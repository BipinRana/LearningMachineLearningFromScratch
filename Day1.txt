Today, I first started with Data Preprocessing. 
In any machine learning field, I have come to know that first we should be able to play around with numbers. 
To do so, we are using python libraries such as numpy,pandas and matplotlib for visualization.

NumPy is bread and butter for any Python coder who wants to learn Data Science/Machine Learning.
One may wonder, why? Python by itself is a very slow programming language. So, we use Numpy, a Python library which is built by using C.
In it there's only one data type, ndarray, representing a n number of dimensional array.
The ndarray is homogeneous data type meaning that there can be only data type in the array and the number  of data in a list cannot be changed.
In contrast, a python's list is heterogenous and its size is determined during runtime which makes the list operations very slow.

Pandas is another library that is used often for dealing with large numbers of data.
It also helps in filehandling.

Now, moving on to the data preprocessing: 
1) Importing dataset: We use pandas to import large set of data sets that we can use to study, make models  

2) Handling missing data: Sometimes, the data we are handling has some missing values like NaN, 0 and more. These data need to be handled so that there can be precise representations and calculations.
                          To do this, we will use SimpleImputer() from sklearn library. When handling missing data, there are multiple strategies. Some of them include mean, median.

3) Encoding Categorical data: Our dataset also include categorical data. These data are the non-numerical values, like Country= ['Nepal','USA','France']
                              These data even though they are not numbers, they have impact on the data that can be Rent Price, Pollution Index and so on.
                              So, we used several encoding methods to enumerate them. Two of them are LabelEncoder() and OneHotEncoder() which have their own way of encoding the data

4) Splitting dataset to test-set and training-set: When creating a model, we use some % of the dataset to train our model. Meanwhile, the remaining % of the dataset is used to test our model. 
                                                   The % can vary according to the dataset. But I have seen that usually 80% of the dataset is used for training and the rest 20% is used for testing.

5) Feature Scaling: Our dataset contains wide array of numbers. These numbers sometimes can vary alot causing a lot of inconsistencies and inaccuracies in the model. Thus we use various techniques.
                    They include Min-Max scaling, Standard scaling.
                            
